
\documentclass[journal]{IEEEtran}
\usepackage{lmodern}
\usepackage{amsfonts}
%\usepackage{hyperref}

\usepackage{cite}
\ifCLASSINFOpdf
  \usepackage[pdftex]{graphicx}
  \graphicspath{{./img/}}
  \DeclareGraphicsExtensions{.pdf,.jpeg,.png}
\else
\fi
\usepackage{array}
\usepackage{url}
\hyphenation{op-tical net-works semi-conduc-tor av-er-age at-tribute}


\begin{document}
%
% paper title
% can use linebreaks \\ within to get better formatting as desired
% Do not put math or special symbols in the title.
\title{Self Organizing Feature Maps}

\author{Evan~Verworn~(4582938)~\textless{}ev09qz@brocku.ca\textgreater% <-this % stops a space
%\thanks{M. Shell is with the Department
%of Electrical and Computer Engineering, Georgia Institute of Technology, Atlanta,
%GA, 30332 USA e-mail: (see http://www.michaelshell.org/contact.html).}% <-this % stops a space
%\thanks{J. Doe and J. Doe are with Anonymous University.}% <-this % stops a space
%\thanks{Manuscript received April 19, 2005; revised December 27, 2012.}
}


% make the title area
\maketitle

% As a general rule, do not put math, special symbols or citations
% in the abstract or keywords.
\begin{abstract}
This report is to show the uses of a Self Organizing Feature Map in cases of clustering. The Self Organizing Maps are then anaylised using density based clustering
modeling and cluster validation using the Dunn Index.
\end{abstract}

\IEEEpeerreviewmaketitle

\section{Introduction}
\IEEEPARstart{T}{he} purpose of this assignment is for use to show that we understand
Self Organizing Maps and methods of performing analysis on the results. Self Organizing Maps also are useful for reducing the curse of dimensionality problem
that arises within other ANNs by eliminating data points that are too similar or 
outliers. 

\section{Problem}
% Clear description of the problem and data being used.
This report will look at two toy problems in image analysis. First a color 
organization problem where a subset of colours are given and the goal is to cluster
the colours into similar and dis-similar groups. The second problem will be a type of 
Optical Character Recognition (or OCR), wherein the goal of this problem is to find
the number of different characters presented to it.

\subsection{Problem I: Colour Map}
As described above the purpose of this problem is to group similar colours. For the
purposes of this problem I am limiting my colours to a subset of 13. These 13 colours
are the built in colours of the Java AWT library, they are as follows.
% Color.BLACK, Color.BLUE, Color.CYAN, Color.DARK_GRAY, Color.GRAY, 
%            Color.GREEN, Color.LIGHT_GRAY, Color.MAGENTA, Color.ORANGE, Color.PINK,
%            Color.RED, Color.WHITE, Color.YELLOW  
Black, Blue, Cyan, Dark Gray, Gray, Green, Light Gray, Magenta, Orange, Pink, Red,
White and Yellow.

These colours were chosen because of their similar and dis-similar natures. The 
solution application should put the colours White and Black into different
clusters but possibily cluster White and Light Gray into a similar grouping. 

All of the above colours were transformed into their Red, Green and Blue components
which then became the input data for this problem. All values are in the $\mathbb{R}$
domain ranging from 0 to 1.

\subsection{Problem II: OCR}
Optical Character Recognition is a problem traditionally solved with
an adaline Nerual Network. However in this case instead of attempting to classify
an image as a letter, the goal will be to determine the number of different letters
 shown to it and group similar characters together.  

For the purposes of this problem I will be giving it the first 6 letters of the 
alphabet (A through F). All letters will be in their capital letter forms, centered
in the middle of the image with a plain white background. These images will then be
preprocessed before anaylisis begins. 

The first 8 attributes of the input vector will be as follows.

\begin{description}
  \item[Average Colour] \hfill \\ The average Red, Green and Blue (RGB) colour of the entire image.
  \item[Standard Deviation] \hfill \\  The standard deviation of the RGB colours of the entire image.
  \item[Average Singular Colour] \hfill \\  This attribute is the average of the Red/Green/Blue channel for the entire image. Each of the colours will have their own value in the final vector.
  \item[Standard Deviation of a Singular Colour] \hfill \\  The standard deviation of the Red/Green/Blue channel for the entire image. Each of the colours have their own value in the final vector.
\end{description}

In addition to these values the image will be split into 9 equal sections. With each
subsection having an \emph{Average Colour} attribute along with a \emph{Standard Deviation} of the RGB value. 

In total this means that each image will have a 26 long input vector, with all values
in the $\mathbb{R}$ domain and in the range of 0 to 1. The problem has a much higher 
dimensionality and also provides the challenge of determining how to best visulize the
problem. As the number of features is so high, this means that the no matter what
colour mapping technique is used no one plot can properally convey the state of the
solution.

\subsection{Using Neural Networks}
% Why neural networks?
Neural Networks are a method of artifical intellegence that are easy to implement and
excells in processing noisy data, or in this case \emph{clustering} noisy data. Using 
Self Organizing Feature Maps (SOFM or SOM), we can show
the nerual net our data and see what patterns emerge. Through statistical anaylisis
we can then see what how accurate our representation is and use this to determine
how many different groupings of input data we have and how hard it will be for other 
AI systems (like adaline networks) to determine the difference between them.

\section{Neural Networks}
In this section the type of Neural Network that will be described will be that of
Self Organizing Feature Maps or SOFMs. 

SOFMs are a type of unsupervised computational methods for visulizing high dimensional
data\cite{SOM_KO}. These SOMs are also sometimes called Kohonen Maps after their 
creator T.~Kohonen.

% What is the kohonen Maps
A Kohonen Map is a latice of nodes that each hold a collection of weights. Each of 
these nodes are aware of it's location and it's neighbours in the n-dimensional map
it exists in. These weights that each node holds are used to organize and cluster
the input data on this latice. Each node is aware, or directly connected to the input
vectors that are being grouped onto the map.

% The Learning Rule
Each node is updated using the following highlevel algorithm. Given a input vector $I$.

% find node closest to input weight (see the \Calculating Weight Distance section)
% pull node closer to input weight
% pull neighbours closer to input weight
% decay learning rate and neighbourhood

\begin{enumerate}
	\item find the node with the weights that are the closest in eucledian space
to that of the input node $I$.
	\item pull the weights of this found node closer to that of $I$.
	\item pull the neighbours closer to the given $I$ dependant on the amount of
distance from the above node.
	\item decay the neighbourhood size and learning rate.
\end{enumerate}

And repeat the above for each input vector that you'd like to cluster/group. 

Now that the highlevel algorithm is stated the following is a more detailed step
for step version of the process of the SOM.

\subsection{Finding the closest node}
When the 'closest' node is said, it is meant using the function.

$$ distance = \sqrt{ (w_0 - p_0)^2 + (w_1 - p_1)^2 + \ldots + (w_n - p_n)^2 } $$

Where $w_i$ is the weight of the input vector and $p_i$ is the weight of the node
we're testing. 

The node with the lowest distance is the node we will pick for the weight update.

\subsection{Weight Update}

The weight update function is simple, the purpose is to just pull the weight of the node closer to that of the input vector. I do this but using the following equation.

$$ W_i = W_i + (I_i- W_i) * L * D $$

Where $W_i$ is the weight for the node we are currently updating, $I_i$ is the weight of the input node we're trying to move closer to. $L$ is the learning rate constant, and $D$ is the distance we are from our closest node. In this case the value of $D = 1.0$.

\subsection{Neighbourhood Update}

If we only changed the single node we wouldn't be able to cluster similar vectors together. We encourage alike vectors to be placed together by updating a neighbourhood of vectors toward a given input vector. In the above weight update equation the variable $D$ was mentioned. The following is the function that is used to calculate $D$.

$$ d = \sqrt{min(|\Delta{}x|, w - |\Delta{}x|)^2 + \\ min(|\Delta{}y|, h - |\Delta{}y|)^2} $$
$$ D = N / (d + N) $$

Where $(x,y)$ is the location of the node in the latice we are measuring the distance for. $w$ is the width of the map and $h$ being the height of the layer. $N$ is the neighbourhood size constant and affect how large of an area is pulled.

\subsection{Constant Decay}

As sections of the map are `pulled' toward the group of input vectors we'd like for the map to slowly converge to one state. This is achomplished by modifying the neighbourhood size constant ($N$) by slowly decresing it's value. Eventually it would reach $1$ thus meaning the weight update function would only update itself, no one else.
This would lead to the map converging on a single state, exactly what we want.

An example of an equation of decay that could be used.

$$ N = N_s * e^{(-e/G)} + 1 $$

Where $N$ is the neighbourhood size constant, $N_s$ is the starting neighbourhood constant, $e$ is the current epoch the SOM is on and $G$ being the maximum number of epochs.

\section{Methods of Analysis}

\subsection{Density Based Clustering}
Density based clustering is a method of determining how many clusters the final solution has. Clusters are defined by areas with a high density of data points.
A common algorithm that is used in these cases is called DBSCAN. 

DBSCAN works by starting to traverse the data points, it then looks in the immediate area ($\epsilon$) for other data points. If it is able to find at least $\mu$ data points then the current point should be considered part of a cluster.  

\begin{figure}[!t]
\centering
\includegraphics[width=2.5in]{DBSCAN}
\caption{A is a cluster center, B and C are reachable from A but not dense enough to be considered part of the cluster. Point N is a noise point and is not correlated to any other node.}
\label{fig:dbscan_ex}
\end{figure}

See Figure ~\ref{fig:dbscan_ex} for a visual example of the algorithm.

\subsection{Cluster Validation Using Dunn Index}
Similar to the DBSCAN the Dunn Index tries to identify tightly packed clusters, giving
a better score to clusters that are not spread out as far.

The score is based off of the ratio from the spread of a single cluster to the 
distance of the centoid of the closest different cluster centoid. The equation of the Dunn Index is as follows.

$$ D = \min_{i=1..n_c} (\min_{i=i+1..n_c}(d(i,j)/\max_{k_1..n_c} diam(c_k))) $$

A very dense confusing line.

Where $D$ is the Dunn Index, $d(i,j)$ is the inter-distance between the two clusters, $diam(c_k)$ is the intra-cluster size (size of the area of cluster).

The lower the $D$ value the better the representation of the clusters are.


\section{Results and Discussion}

\begin{table}[!t]
\renewcommand{\arraystretch}{1.3}
\caption{Part I Parameters}
\label{P1}
\centering
\begin{tabular}{|c||c|}
\hline
Map Dimensions & 200\\
\hline
Learning Rate & 0.6\\
\hline
Neighbourhood Size & 40\\
\hline
Maximum Epochs & 500\\
\hline
\# Colours & 13\\
\hline
\end{tabular}
\end{table}


\begin{figure}[!t]
\centering
\includegraphics[width=2.5in]{P1}
\caption{Part 1 mapping results}
\label{p1_results}
\end{figure}

See Figure~\ref{p1_results} and Table~\ref{P1}


\section{Image Clustering Example}
This part is uncomplete.

% An example of a floating figure using the graphicx package.
% Note that \label must occur AFTER (or within) \caption.
% For figures, \caption should occur after the \includegraphics.
% Note that IEEEtran v1.7 and later has special internal code that
% is designed to preserve the operation of \label within \caption
% even when the captionsoff option is in effect. However, because
% of issues like this, it may be the safest practice to put all your
% \label just after \caption rather than within \caption{}.
%
% Reminder: the "draftcls" or "draftclsnofoot", not "draft", class
% option should be used if it is desired that the figures are to be
% displayed while in draft mode.
%
%\begin{figure}[!t]
%\centering
%\includegraphics[width=2.5in]{myfigure}
% where an .eps filename suffix will be assumed under latex, 
% and a .pdf suffix will be assumed for pdflatex; or what has been declared
% via \DeclareGraphicsExtensions.
%\caption{Simulation Results.}
%\label{fig_sim}
%\end{figure}

% Note that IEEE typically puts floats only at the top, even when this
% results in a large percentage of a column being occupied by floats.


% An example of a double column floating figure using two subfigures.
% (The subfig.sty package must be loaded for this to work.)
% The subfigure \label commands are set within each subfloat command,
% and the \label for the overall figure must come after \caption.
% \hfil is used as a separator to get equal spacing.
% Watch out that the combined width of all the subfigures on a 
% line do not exceed the text width or a line break will occur.
%
%\begin{figure*}[!t]
%\centering
%\subfloat[Case I]{\includegraphics[width=2.5in]{box}%
%\label{fig_first_case}}
%\hfil
%\subfloat[Case II]{\includegraphics[width=2.5in]{box}%
%\label{fig_second_case}}
%\caption{Simulation results.}
%\label{fig_sim}
%\end{figure*}
%
% Note that often IEEE papers with subfigures do not employ subfigure
% captions (using the optional argument to \subfloat[]), but instead will
% reference/describe all of them (a), (b), etc., within the main caption.


% An example of a floating table. Note that, for IEEE style tables, the 
% \caption command should come BEFORE the table. Table text will default to
% \footnotesize as IEEE normally uses this smaller font for tables.
% The \label must come after \caption as always.
%


% Note that IEEE does not put floats in the very first column - or typically
% anywhere on the first page for that matter. Also, in-text middle ("here")
% positioning is not used. Most IEEE journals use top floats exclusively.
% Note that, LaTeX2e, unlike IEEE journals, places footnotes above bottom
% floats. This can be corrected via the \fnbelowfloat command of the
% stfloats package.



\section{Conclusion}
Unconclusive.





% if have a single appendix:
%\appendix[Proof of the Zonklar Equations]
% or
%\appendix  % for no appendix heading
% do not use \section anymore after \appendix, only \section*
% is possibly needed

% use appendices with more than one appendix
% then use \section to start each appendix
% you must declare a \section before using any
% \subsection or using \label (\appendices by itself
% starts a section numbered zero.)
%


%\appendices
%\section{Proof of the First Zonklar Equation}
%Appendix one text goes here.

% you can choose not to have a title for an appendix
% if you want by leaving the argument blank
%\section{}
%Appendix two text goes here.


% use section* for acknowledgement
%\section*{Acknowledgment}


%The authors would like to thank...


% Can use something like this to put references on a page
% by themselves when using endfloat and the captionsoff option.
\ifCLASSOPTIONcaptionsoff
  \newpage
\fi



% trigger a \newpage just before the given reference
% number - used to balance the columns on the last page
% adjust value as needed - may need to be readjusted if
% the document is modified later
%\IEEEtriggeratref{8}
% The "triggered" command can be changed if desired:
%\IEEEtriggercmd{\enlargethispage{-5in}}

% references section

% can use a bibliography generated by BibTeX as a .bbl file
% BibTeX documentation can be easily obtained at:
% http://www.ctan.org/tex-archive/biblio/bibtex/contrib/doc/
% The IEEEtran BibTeX style support page is at:
% http://www.michaelshell.org/tex/ieeetran/bibtex/
%\bibliographystyle{IEEEtran}
% argument is your BibTeX string definitions and bibliography database(s)
%\bibliography{IEEEabrv,../bib/paper}
%
% <OR> manually copy in the resultant .bbl file
% set second argument of \begin to the number of references
% (used to reserve space for the reference number labels box)
\begin{thebibliography}{1}

\bibitem{SOM_KO}
Kohonen, T. (2001). Self-Organizing Maps. Third, extended edition. Springer, Berlin.

\bibitem{Dunn}
Dunn, J. (1974). "Well separated clusters and optimal fuzzy partitions". Journal of Cybernetics 4

\end{thebibliography}

% You can push biographies down or up by placing
% a \vfill before or after them. The appropriate
% use of \vfill depends on what kind of text is
% on the last page and whether or not the columns
% are being equalized.

%\vfill

% Can be used to pull up biographies so that the bottom of the last one
% is flush with the other column.
%\enlargethispage{-5in}



% that's all folks
\end{document}


